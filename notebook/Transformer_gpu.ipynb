{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_gpu.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMB1sHJBy6leHNtNTlF1IW4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5vMpP8CRbG8",
        "outputId": "c1dc5ca9-5b87-4bdf-d00f-54675a17387d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB6_LgyES2Q3",
        "outputId": "0b3ead10-092c-40a6-8e2e-a0314d4161a3"
      },
      "source": [
        "!apt install aptitude\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3==0.6\n",
        "!pip install japanize_matplotlib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  aptitude-common libcgi-fast-perl libcgi-pm-perl libclass-accessor-perl\n",
            "  libcwidget3v5 libencode-locale-perl libfcgi-perl libhtml-parser-perl\n",
            "  libhtml-tagset-perl libhttp-date-perl libhttp-message-perl libio-html-perl\n",
            "  libio-string-perl liblwp-mediatypes-perl libparse-debianchangelog-perl\n",
            "  libsigc++-2.0-0v5 libsub-name-perl libtimedate-perl liburi-perl libxapian30\n",
            "Suggested packages:\n",
            "  aptitude-doc-en | aptitude-doc apt-xapian-index debtags tasksel\n",
            "  libcwidget-dev libdata-dump-perl libhtml-template-perl libxml-simple-perl\n",
            "  libwww-perl xapian-tools\n",
            "The following NEW packages will be installed:\n",
            "  aptitude aptitude-common libcgi-fast-perl libcgi-pm-perl\n",
            "  libclass-accessor-perl libcwidget3v5 libencode-locale-perl libfcgi-perl\n",
            "  libhtml-parser-perl libhtml-tagset-perl libhttp-date-perl\n",
            "  libhttp-message-perl libio-html-perl libio-string-perl\n",
            "  liblwp-mediatypes-perl libparse-debianchangelog-perl libsigc++-2.0-0v5\n",
            "  libsub-name-perl libtimedate-perl liburi-perl libxapian30\n",
            "0 upgraded, 21 newly installed, 0 to remove and 10 not upgraded.\n",
            "Need to get 3,877 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude-common all 0.8.10-6ubuntu1 [1,014 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigc++-2.0-0v5 amd64 2.10.0-2 [10.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcwidget3v5 amd64 0.5.17-7 [286 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxapian30 amd64 1.4.5-1ubuntu0.1 [631 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude amd64 0.8.10-6ubuntu1 [1,269 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tagset-perl all 3.20-3 [12.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 liburi-perl all 1.73-1 [77.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-parser-perl amd64 3.72-3build1 [85.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-pm-perl all 4.38-1 [185 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfcgi-perl amd64 0.78-2build1 [32.8 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-fast-perl all 1:2.13-1 [9,940 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsub-name-perl amd64 0.21-1build1 [11.6 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libclass-accessor-perl all 0.51-1 [21.2 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-date-perl all 6.02-1 [10.4 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-mediatypes-perl all 6.02-1 [21.7 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-message-perl all 6.14-1 [72.1 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-string-perl all 1.08-3 [11.1 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libparse-debianchangelog-perl all 1.2.0-12 [49.5 kB]\n",
            "Fetched 3,877 kB in 2s (2,271 kB/s)\n",
            "Selecting previously unselected package aptitude-common.\n",
            "(Reading database ... 146442 files and directories currently installed.)\n",
            "Preparing to unpack .../00-aptitude-common_0.8.10-6ubuntu1_all.deb ...\n",
            "Unpacking aptitude-common (0.8.10-6ubuntu1) ...\n",
            "Selecting previously unselected package libsigc++-2.0-0v5:amd64.\n",
            "Preparing to unpack .../01-libsigc++-2.0-0v5_2.10.0-2_amd64.deb ...\n",
            "Unpacking libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n",
            "Selecting previously unselected package libcwidget3v5:amd64.\n",
            "Preparing to unpack .../02-libcwidget3v5_0.5.17-7_amd64.deb ...\n",
            "Unpacking libcwidget3v5:amd64 (0.5.17-7) ...\n",
            "Selecting previously unselected package libxapian30:amd64.\n",
            "Preparing to unpack .../03-libxapian30_1.4.5-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n",
            "Selecting previously unselected package aptitude.\n",
            "Preparing to unpack .../04-aptitude_0.8.10-6ubuntu1_amd64.deb ...\n",
            "Unpacking aptitude (0.8.10-6ubuntu1) ...\n",
            "Selecting previously unselected package libhtml-tagset-perl.\n",
            "Preparing to unpack .../05-libhtml-tagset-perl_3.20-3_all.deb ...\n",
            "Unpacking libhtml-tagset-perl (3.20-3) ...\n",
            "Selecting previously unselected package liburi-perl.\n",
            "Preparing to unpack .../06-liburi-perl_1.73-1_all.deb ...\n",
            "Unpacking liburi-perl (1.73-1) ...\n",
            "Selecting previously unselected package libhtml-parser-perl.\n",
            "Preparing to unpack .../07-libhtml-parser-perl_3.72-3build1_amd64.deb ...\n",
            "Unpacking libhtml-parser-perl (3.72-3build1) ...\n",
            "Selecting previously unselected package libcgi-pm-perl.\n",
            "Preparing to unpack .../08-libcgi-pm-perl_4.38-1_all.deb ...\n",
            "Unpacking libcgi-pm-perl (4.38-1) ...\n",
            "Selecting previously unselected package libfcgi-perl.\n",
            "Preparing to unpack .../09-libfcgi-perl_0.78-2build1_amd64.deb ...\n",
            "Unpacking libfcgi-perl (0.78-2build1) ...\n",
            "Selecting previously unselected package libcgi-fast-perl.\n",
            "Preparing to unpack .../10-libcgi-fast-perl_1%3a2.13-1_all.deb ...\n",
            "Unpacking libcgi-fast-perl (1:2.13-1) ...\n",
            "Selecting previously unselected package libsub-name-perl.\n",
            "Preparing to unpack .../11-libsub-name-perl_0.21-1build1_amd64.deb ...\n",
            "Unpacking libsub-name-perl (0.21-1build1) ...\n",
            "Selecting previously unselected package libclass-accessor-perl.\n",
            "Preparing to unpack .../12-libclass-accessor-perl_0.51-1_all.deb ...\n",
            "Unpacking libclass-accessor-perl (0.51-1) ...\n",
            "Selecting previously unselected package libencode-locale-perl.\n",
            "Preparing to unpack .../13-libencode-locale-perl_1.05-1_all.deb ...\n",
            "Unpacking libencode-locale-perl (1.05-1) ...\n",
            "Selecting previously unselected package libtimedate-perl.\n",
            "Preparing to unpack .../14-libtimedate-perl_2.3000-2_all.deb ...\n",
            "Unpacking libtimedate-perl (2.3000-2) ...\n",
            "Selecting previously unselected package libhttp-date-perl.\n",
            "Preparing to unpack .../15-libhttp-date-perl_6.02-1_all.deb ...\n",
            "Unpacking libhttp-date-perl (6.02-1) ...\n",
            "Selecting previously unselected package libio-html-perl.\n",
            "Preparing to unpack .../16-libio-html-perl_1.001-1_all.deb ...\n",
            "Unpacking libio-html-perl (1.001-1) ...\n",
            "Selecting previously unselected package liblwp-mediatypes-perl.\n",
            "Preparing to unpack .../17-liblwp-mediatypes-perl_6.02-1_all.deb ...\n",
            "Unpacking liblwp-mediatypes-perl (6.02-1) ...\n",
            "Selecting previously unselected package libhttp-message-perl.\n",
            "Preparing to unpack .../18-libhttp-message-perl_6.14-1_all.deb ...\n",
            "Unpacking libhttp-message-perl (6.14-1) ...\n",
            "Selecting previously unselected package libio-string-perl.\n",
            "Preparing to unpack .../19-libio-string-perl_1.08-3_all.deb ...\n",
            "Unpacking libio-string-perl (1.08-3) ...\n",
            "Selecting previously unselected package libparse-debianchangelog-perl.\n",
            "Preparing to unpack .../20-libparse-debianchangelog-perl_1.2.0-12_all.deb ...\n",
            "Unpacking libparse-debianchangelog-perl (1.2.0-12) ...\n",
            "Setting up libhtml-tagset-perl (3.20-3) ...\n",
            "Setting up libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n",
            "Setting up libencode-locale-perl (1.05-1) ...\n",
            "Setting up libtimedate-perl (2.3000-2) ...\n",
            "Setting up libio-html-perl (1.001-1) ...\n",
            "Setting up aptitude-common (0.8.10-6ubuntu1) ...\n",
            "Setting up liblwp-mediatypes-perl (6.02-1) ...\n",
            "Setting up liburi-perl (1.73-1) ...\n",
            "Setting up libhtml-parser-perl (3.72-3build1) ...\n",
            "Setting up libcgi-pm-perl (4.38-1) ...\n",
            "Setting up libio-string-perl (1.08-3) ...\n",
            "Setting up libsub-name-perl (0.21-1build1) ...\n",
            "Setting up libfcgi-perl (0.78-2build1) ...\n",
            "Setting up libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n",
            "Setting up libclass-accessor-perl (0.51-1) ...\n",
            "Setting up libhttp-date-perl (6.02-1) ...\n",
            "Setting up libcgi-fast-perl (1:2.13-1) ...\n",
            "Setting up libparse-debianchangelog-perl (1.2.0-12) ...\n",
            "Setting up libhttp-message-perl (6.14-1) ...\n",
            "Setting up libcwidget3v5:amd64 (0.5.17-7) ...\n",
            "Setting up aptitude (0.8.10-6ubuntu1) ...\n",
            "update-alternatives: using /usr/bin/aptitude-curses to provide /usr/bin/aptitude (aptitude) in auto mode\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.7)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.12)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3)\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.7)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.12)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3)\n",
            "The following NEW packages will be installed:\n",
            "  file libmagic-mgc{a} libmagic1{a} libmecab-dev libmecab2{a} mecab mecab-ipadic{a} mecab-ipadic-utf8 mecab-jumandic{a} mecab-jumandic-utf8{a} mecab-utils{a} \n",
            "0 packages upgraded, 11 newly installed, 0 to remove and 10 not upgraded.\n",
            "Need to get 29.3 MB of archives. After unpacking 282 MB will be used.\n",
            "Get: 1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get: 2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get: 3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.4 [22.1 kB]\n",
            "Get: 4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab2 amd64 0.996-5 [257 kB]\n",
            "Get: 5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab-dev amd64 0.996-5 [308 kB]\n",
            "Get: 6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-utils amd64 0.996-5 [4,856 B]\n",
            "Get: 7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic-utf8 all 7.0-20130310-4 [16.2 MB]\n",
            "Get: 8 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic all 7.0-20130310-4 [2,212 B]\n",
            "Get: 9 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic all 2.7.0-20070801+main-1 [12.1 MB]\n",
            "Get: 10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab amd64 0.996-5 [132 kB]\n",
            "Get: 11 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic-utf8 all 2.7.0-20070801+main-1 [3,522 B]\n",
            "Fetched 29.3 MB in 3s (11.0 MB/s)\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "(Reading database ... 146901 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../01-libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package file.\n",
            "Preparing to unpack .../02-file_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking file (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmecab2:amd64.\n",
            "Preparing to unpack .../03-libmecab2_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab2:amd64 (0.996-5) ...\n",
            "Selecting previously unselected package libmecab-dev.\n",
            "Preparing to unpack .../04-libmecab-dev_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab-dev (0.996-5) ...\n",
            "Selecting previously unselected package mecab-utils.\n",
            "Preparing to unpack .../05-mecab-utils_0.996-5_amd64.deb ...\n",
            "Unpacking mecab-utils (0.996-5) ...\n",
            "Selecting previously unselected package mecab-jumandic-utf8.\n",
            "Preparing to unpack .../06-mecab-jumandic-utf8_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-jumandic.\n",
            "Preparing to unpack .../07-mecab-jumandic_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-ipadic.\n",
            "Preparing to unpack .../08-mecab-ipadic_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Selecting previously unselected package mecab.\n",
            "Preparing to unpack .../09-mecab_0.996-5_amd64.deb ...\n",
            "Unpacking mecab (0.996-5) ...\n",
            "Selecting previously unselected package mecab-ipadic-utf8.\n",
            "Preparing to unpack .../10-mecab-ipadic-utf8_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Setting up libmecab2:amd64 (0.996-5) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up mecab-utils (0.996-5) ...\n",
            "Setting up mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up libmecab-dev (0.996-5) ...\n",
            "Setting up file (1:5.32-2ubuntu0.4) ...\n",
            "Setting up mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Compiling Juman dictionary for Mecab.\n",
            "reading /usr/share/mecab/dic/juman/unk.def ... 37\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/Noun.keishiki.csv ... 8\n",
            "reading /usr/share/mecab/dic/juman/Noun.hukusi.csv ... 81\n",
            "reading /usr/share/mecab/dic/juman/Wikipedia.csv ... 167709\n",
            "reading /usr/share/mecab/dic/juman/Special.csv ... 158\n",
            "reading /usr/share/mecab/dic/juman/Rengo.csv ... 1118\n",
            "reading /usr/share/mecab/dic/juman/ContentW.csv ... 551145\n",
            "reading /usr/share/mecab/dic/juman/Suffix.csv ... 2128\n",
            "reading /usr/share/mecab/dic/juman/Prefix.csv ... 90\n",
            "reading /usr/share/mecab/dic/juman/Assert.csv ... 34\n",
            "reading /usr/share/mecab/dic/juman/Auto.csv ... 18931\n",
            "reading /usr/share/mecab/dic/juman/Noun.suusi.csv ... 49\n",
            "reading /usr/share/mecab/dic/juman/Demonstrative.csv ... 97\n",
            "reading /usr/share/mecab/dic/juman/Noun.koyuu.csv ... 7964\n",
            "reading /usr/share/mecab/dic/juman/Postp.csv ... 108\n",
            "reading /usr/share/mecab/dic/juman/AuxV.csv ... 593\n",
            "reading /usr/share/mecab/dic/juman/Emoticon.csv ... 972\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/matrix.def ... 1876x1876\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab (0.996-5) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-jumandic (7.0-20130310-4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "                            \n",
            "Collecting mecab-python3==0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/7f/f98371035a0171abf95f9893eabf915f8a3199d005fed3cd69cc122fed40/mecab-python3-0.6.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: mecab-python3\n",
            "  Building wheel for mecab-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mecab-python3: filename=mecab_python3-0.6-cp36-cp36m-linux_x86_64.whl size=155491 sha256=bd37247293003b57573d8eb0cd662d93ee6b2425c9fd9160bc0f456697c10dc8\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/51/5b/987888cacaf8bb25982ef4569261f68debe85b7587c5563c79\n",
            "Successfully built mecab-python3\n",
            "Installing collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-0.6\n",
            "Collecting japanize_matplotlib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/85/08a4b7fe8987582d99d9bb7ad0ff1ec75439359a7f9690a0dbf2dbf98b15/japanize-matplotlib-1.1.3.tar.gz (4.1MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from japanize_matplotlib) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize_matplotlib) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize_matplotlib) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize_matplotlib) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize_matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize_matplotlib) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->japanize_matplotlib) (1.15.0)\n",
            "Building wheels for collected packages: japanize-matplotlib\n",
            "  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for japanize-matplotlib: filename=japanize_matplotlib-1.1.3-cp36-none-any.whl size=4120276 sha256=119e51778f3a9aeee6faa7b1c616e064c3a02238c98075e03b1abcc32f983a7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/d9/a2/f907d50b32a2d2008ce5d691d30fb6569c2c93eefcfde55202\n",
            "Successfully built japanize-matplotlib\n",
            "Installing collected packages: japanize-matplotlib\n",
            "Successfully installed japanize-matplotlib-1.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-KvtVfNUx-e",
        "outputId": "ab1c5cb8-d337-44a8-b1ae-c5aa7a878f2e"
      },
      "source": [
        "cd /content/drive/MyDrive/Transformer/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Transformer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNMU7q1sTAmL",
        "outputId": "df7c6991-a3b8-418e-da7e-f312fabf1b10"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import MeCab\n",
        "\n",
        "import preprocess_utils\n",
        "import model\n",
        "import weight_utils\n",
        "\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab0hKwBPTh40"
      },
      "source": [
        "## 日英翻訳データ ダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqdOhov_TUA3",
        "outputId": "58da1ca2-59bf-4543-f149-cd05a2e1faea"
      },
      "source": [
        "!wget http://www.manythings.org/anki/jpn-eng.zip\n",
        "!unzip ./jpn-eng.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-20 12:49:08--  http://www.manythings.org/anki/jpn-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 172.67.173.198, 104.21.55.222, 2606:4700:3036::ac43:adc6, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|172.67.173.198|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2824891 (2.7M) [application/zip]\n",
            "Saving to: ‘jpn-eng.zip.1’\n",
            "\n",
            "jpn-eng.zip.1       100%[===================>]   2.69M  5.09MB/s    in 0.5s    \n",
            "\n",
            "2021-02-20 12:49:09 (5.09 MB/s) - ‘jpn-eng.zip.1’ saved [2824891/2824891]\n",
            "\n",
            "Archive:  ./jpn-eng.zip\n",
            "replace jpn.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: jpn.txt                 \n",
            "  inflating: _about.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTCuXr28VPaK"
      },
      "source": [
        "## データ読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "472kMBJpVKMB",
        "outputId": "5a575f2a-4085-460d-e34d-2ab4b712dfdc"
      },
      "source": [
        "dataset = preprocess_utils.CreateData(\n",
        "    corpus_path = './jpn.txt',\n",
        "    do_shuffle=True,\n",
        "    seed_value=123,\n",
        "    split_percent=0.95 # 学習データの割合\n",
        ")\n",
        "\n",
        "train_source, train_target, test_source, test_target, train_licence, test_licence = dataset.split_data()\n",
        "\n",
        "print('**** Amount of data ****')\n",
        "print('train_source： ', len(train_source))\n",
        "print('train_target： ', len(train_target))\n",
        "print('test_source： ', len(test_source))\n",
        "print('test_target： ', len(test_target))\n",
        "print('\\n')\n",
        "print('**** Train data example ****')\n",
        "print('Source Example： ', train_source[0])\n",
        "print('Target Example： ', train_target[0])\n",
        "print('Licence： ', train_licence[0])\n",
        "print('\\n')\n",
        "print('**** Test data example ****')\n",
        "print('Source Example： ', test_source[0])\n",
        "print('Target Example： ', test_target[0])\n",
        "print('Licence： ', test_licence[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**** Amount of data ****\n",
            "train_source：  63588\n",
            "train_target：  63588\n",
            "test_source：  3347\n",
            "test_target：  3347\n",
            "\n",
            "\n",
            "**** Train data example ****\n",
            "Source Example：  Her eyes are blue.\n",
            "Target Example：  彼女は目が青い。\n",
            "Licence：  CC-BY 2.0 (France) Attribution: tatoeba.org #471891 (blay_paul) & #472086 (bunbuku)\n",
            "\n",
            "\n",
            "\n",
            "**** Test data example ****\n",
            "Source Example：  Do you still need tea?\n",
            "Target Example：  まだお茶が要りますか。\n",
            "Licence：  CC-BY 2.0 (France) Attribution: tatoeba.org #32684 (Eldad) & #195510 (bunbuku)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a6E6BB_VZL4"
      },
      "source": [
        "## 前処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oALRIMt7VVct"
      },
      "source": [
        "BATCH_SIZE = 64 # バッチサイズ\n",
        "MAX_LENGTH = 60 # シーケンスの長さ\n",
        "USE_TPU = False # TPUを使うか\n",
        "BUFFER_SIZE = 50000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN3hH539VcMl"
      },
      "source": [
        "train_dataset = preprocess_utils.PreprocessData(\n",
        "    mecab = MeCab.Tagger(\"-Ochasen\"),\n",
        "    source_data = train_source,\n",
        "    target_data = train_target,\n",
        "    max_length = MAX_LENGTH,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    test_flag = False,\n",
        "    train_dataset = None,\n",
        ")\n",
        "\n",
        "train_dataset.preprocess_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN2XqebnsEWq"
      },
      "source": [
        "test_dataset = preprocess_utils.PreprocessData(\n",
        "    mecab = MeCab.Tagger(\"-Ochasen\"),\n",
        "    source_data = test_source,\n",
        "    target_data = test_target,\n",
        "    max_length = MAX_LENGTH,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    test_flag = True,\n",
        "    train_dataset = train_dataset\n",
        ")\n",
        "\n",
        "test_dataset.preprocess_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEXgXyuZVd1e"
      },
      "source": [
        "if USE_TPU:\n",
        "  tpu_grpc_url = \"grpc://\" + os.environ[\"COLAB_TPU_ADDR\"]\n",
        "  tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu_grpc_url)\n",
        "  tf.config.experimental_connect_to_cluster(tpu_cluster_resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver)    \n",
        "  strategy = tf.distribute.experimental.TPUStrategy(tpu_cluster_resolver)\n",
        "\n",
        "trainset = tf.data.Dataset.from_tensor_slices((train_dataset.source_vector, train_dataset.target_vector))\n",
        "trainset = trainset.map(lambda source, target: (tf.cast(source, tf.int64), tf.cast(target, tf.int64))).shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "if USE_TPU:\n",
        "  trainset = strategy.experimental_distribute_dataset(trainset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpftO0LwtYWk"
      },
      "source": [
        "if USE_TPU:\n",
        "  PREDICT_BATCH_SIZE = 8\n",
        "  testset = tf.data.Dataset.from_tensor_slices((test_dataset.source_vector, test_dataset.target_vector))\n",
        "  testset = testset.map(lambda source, target: (tf.cast(source, tf.int64), tf.cast(target, tf.int64))).shuffle(buffer_size=50000).batch(PREDICT_BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  testset = testset.take(1)\n",
        "  testset = strategy.experimental_distribute_dataset(testset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRep_ajvVjCk"
      },
      "source": [
        "## モデル定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw_V_nv2VgSd"
      },
      "source": [
        "num_layers=4 # レイヤー数\n",
        "d_model=64 # 中間層の次元数\n",
        "num_heads=4 # Multi Head Attentionのヘッド数\n",
        "dff=2048 # Feed Forward Networkの次元数\n",
        "dropout_rate = 0.1 # ドロップアウト率\n",
        "\n",
        "source_vocab_size = max(train_dataset.source_token.values()) + 1 # source文の語彙数\n",
        "target_vocab_size = max(train_dataset.target_token.values()) + 1 # target文の語彙数"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwonibxOVml0"
      },
      "source": [
        "# 重み初期化\n",
        "def initialize_weight(checkpoint_path, optimizer, transformer, max_length, batch_size, use_tpu=False):\n",
        "\n",
        "  if os.path.exists(checkpoint_path+'.pkl'):\n",
        "    if use_tpu:\n",
        "      number_of_tpu_cores = tpu_cluster_resolver.num_accelerators()['TPU']\n",
        "      initialize_source, initialize_target = [[1]*max_length]*number_of_tpu_cores, [[1]*max_length]*number_of_tpu_cores\n",
        "      initialize_set = tf.data.Dataset.from_tensor_slices((initialize_source, initialize_target))\n",
        "      initialize_set = initialize_set.map(lambda source, target: (tf.cast(source, tf.int64), tf.cast(target, tf.int64))\n",
        "          ).shuffle(buffer_size=BUFFER_SIZE).batch(batch_size).prefetch(\n",
        "              buffer_size=tf.data.experimental.AUTOTUNE\n",
        "          )\n",
        "      initialize_set = strategy.experimental_distribute_dataset(initialize_set)\n",
        "\n",
        "      for inp, tar in initialize_set:\n",
        "        distributed_train_step(inp, tar)\n",
        "\n",
        "    else:\n",
        "      initialize_set = tf.ones([batch_size, max_length], tf.int64)\n",
        "      train_step(initialize_set, initialize_set)\n",
        "    \n",
        "    try:\n",
        "      weight_utils.load_weights_from_pickle(checkpoint_path, optimizer, transformer)\n",
        "    except:\n",
        "      print('Failed to load checkpoints.')\n",
        "\n",
        "  else:\n",
        "    print('No available checkpoints.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ienGzV57Vp97"
      },
      "source": [
        "## 学習実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOp9xZwmVprN",
        "outputId": "ce743df7-3831-4dc2-f097-f98579c38b2e"
      },
      "source": [
        "# Transformer\n",
        "transformer = model.Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          source_vocab_size, target_vocab_size, \n",
        "                          pe_input=source_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)\n",
        "\n",
        "# Learning Rate\n",
        "learning_rate = model.CustomSchedule(d_model)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "\n",
        "# Loss\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "# Loss Function\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "# Metrics\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "# Checkpoint\n",
        "checkpoint_path = \"/content/drive/MyDrive/Transformer/checkpoints/model\"\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  \n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = model.create_masks(inp, tar_inp)\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp, \n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "  train_loss(loss)\n",
        "  train_accuracy(tar_real, predictions)\n",
        "\n",
        "# Initialize Weight\n",
        "initialize_weight(checkpoint_path, optimizer, transformer, MAX_LENGTH, BATCH_SIZE, use_tpu=USE_TPU)\n",
        "\n",
        "EPOCHS = 20\n",
        "batch = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  \n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  \n",
        "  for inp, tar in trainset:\n",
        "    train_step(inp, tar)\n",
        "    \n",
        "    if batch % 50 == 0:\n",
        "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "      \n",
        "    batch+=1\n",
        "      \n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    print('Saving checkpoint for epoch {} at {}'.format(epoch+1, checkpoint_path))\n",
        "    weight_utils.save_weights_as_pickle(checkpoint_path, optimizer, transformer)\n",
        "    \n",
        "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss.result(), \n",
        "                                                train_accuracy.result()))\n",
        "\n",
        "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No available checkpoints.\n",
            "Epoch 1 Batch 0 Loss 1.7123 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 1.7612 Accuracy 0.0044\n",
            "Epoch 1 Batch 100 Loss 1.7402 Accuracy 0.0106\n",
            "Epoch 1 Batch 150 Loss 1.7189 Accuracy 0.0127\n",
            "Epoch 1 Batch 200 Loss 1.6970 Accuracy 0.0138\n",
            "Epoch 1 Batch 250 Loss 1.6656 Accuracy 0.0144\n",
            "Epoch 1 Batch 300 Loss 1.6278 Accuracy 0.0148\n",
            "Epoch 1 Batch 350 Loss 1.5875 Accuracy 0.0151\n",
            "Epoch 1 Batch 400 Loss 1.5438 Accuracy 0.0154\n",
            "Epoch 1 Batch 450 Loss 1.4987 Accuracy 0.0155\n",
            "Epoch 1 Batch 500 Loss 1.4539 Accuracy 0.0157\n",
            "Epoch 1 Batch 550 Loss 1.4138 Accuracy 0.0160\n",
            "Epoch 1 Batch 600 Loss 1.3769 Accuracy 0.0173\n",
            "Epoch 1 Batch 650 Loss 1.3425 Accuracy 0.0191\n",
            "Epoch 1 Batch 700 Loss 1.3086 Accuracy 0.0211\n",
            "Epoch 1 Batch 750 Loss 1.2768 Accuracy 0.0231\n",
            "Epoch 1 Batch 800 Loss 1.2468 Accuracy 0.0251\n",
            "Epoch 1 Batch 850 Loss 1.2184 Accuracy 0.0270\n",
            "Epoch 1 Batch 900 Loss 1.1921 Accuracy 0.0289\n",
            "Epoch 1 Batch 950 Loss 1.1675 Accuracy 0.0306\n",
            "Epoch 1 Loss 1.1481 Accuracy 0.0321\n",
            "Time taken for 1 epoch: 112.54679369926453 secs\n",
            "\n",
            "Epoch 2 Batch 1000 Loss 0.7167 Accuracy 0.0673\n",
            "Epoch 2 Batch 1050 Loss 0.7032 Accuracy 0.0666\n",
            "Epoch 2 Batch 1100 Loss 0.6913 Accuracy 0.0667\n",
            "Epoch 2 Batch 1150 Loss 0.6872 Accuracy 0.0673\n",
            "Epoch 2 Batch 1200 Loss 0.6832 Accuracy 0.0680\n",
            "Epoch 2 Batch 1250 Loss 0.6789 Accuracy 0.0686\n",
            "Epoch 2 Batch 1300 Loss 0.6759 Accuracy 0.0691\n",
            "Epoch 2 Batch 1350 Loss 0.6746 Accuracy 0.0695\n",
            "Epoch 2 Batch 1400 Loss 0.6696 Accuracy 0.0699\n",
            "Epoch 2 Batch 1450 Loss 0.6675 Accuracy 0.0703\n",
            "Epoch 2 Batch 1500 Loss 0.6634 Accuracy 0.0707\n",
            "Epoch 2 Batch 1550 Loss 0.6610 Accuracy 0.0711\n",
            "Epoch 2 Batch 1600 Loss 0.6586 Accuracy 0.0715\n",
            "Epoch 2 Batch 1650 Loss 0.6564 Accuracy 0.0719\n",
            "Epoch 2 Batch 1700 Loss 0.6537 Accuracy 0.0722\n",
            "Epoch 2 Batch 1750 Loss 0.6505 Accuracy 0.0726\n",
            "Epoch 2 Batch 1800 Loss 0.6482 Accuracy 0.0729\n",
            "Epoch 2 Batch 1850 Loss 0.6462 Accuracy 0.0733\n",
            "Epoch 2 Batch 1900 Loss 0.6439 Accuracy 0.0736\n",
            "Epoch 2 Batch 1950 Loss 0.6421 Accuracy 0.0738\n",
            "Epoch 2 Loss 0.6411 Accuracy 0.0741\n",
            "Time taken for 1 epoch: 101.77379488945007 secs\n",
            "\n",
            "Epoch 3 Batch 2000 Loss 0.5807 Accuracy 0.0815\n",
            "Epoch 3 Batch 2050 Loss 0.5744 Accuracy 0.0811\n",
            "Epoch 3 Batch 2100 Loss 0.5741 Accuracy 0.0809\n",
            "Epoch 3 Batch 2150 Loss 0.5735 Accuracy 0.0813\n",
            "Epoch 3 Batch 2200 Loss 0.5720 Accuracy 0.0813\n",
            "Epoch 3 Batch 2250 Loss 0.5727 Accuracy 0.0816\n",
            "Epoch 3 Batch 2300 Loss 0.5701 Accuracy 0.0816\n",
            "Epoch 3 Batch 2350 Loss 0.5680 Accuracy 0.0819\n",
            "Epoch 3 Batch 2400 Loss 0.5659 Accuracy 0.0821\n",
            "Epoch 3 Batch 2450 Loss 0.5647 Accuracy 0.0822\n",
            "Epoch 3 Batch 2500 Loss 0.5640 Accuracy 0.0825\n",
            "Epoch 3 Batch 2550 Loss 0.5637 Accuracy 0.0827\n",
            "Epoch 3 Batch 2600 Loss 0.5624 Accuracy 0.0830\n",
            "Epoch 3 Batch 2650 Loss 0.5613 Accuracy 0.0831\n",
            "Epoch 3 Batch 2700 Loss 0.5602 Accuracy 0.0833\n",
            "Epoch 3 Batch 2750 Loss 0.5596 Accuracy 0.0835\n",
            "Epoch 3 Batch 2800 Loss 0.5579 Accuracy 0.0836\n",
            "Epoch 3 Batch 2850 Loss 0.5570 Accuracy 0.0838\n",
            "Epoch 3 Batch 2900 Loss 0.5557 Accuracy 0.0839\n",
            "Epoch 3 Batch 2950 Loss 0.5548 Accuracy 0.0842\n",
            "Epoch 3 Loss 0.5539 Accuracy 0.0843\n",
            "Time taken for 1 epoch: 100.8403389453888 secs\n",
            "\n",
            "Epoch 4 Batch 3000 Loss 0.5075 Accuracy 0.0896\n",
            "Epoch 4 Batch 3050 Loss 0.4926 Accuracy 0.0892\n",
            "Epoch 4 Batch 3100 Loss 0.4989 Accuracy 0.0894\n",
            "Epoch 4 Batch 3150 Loss 0.4994 Accuracy 0.0897\n",
            "Epoch 4 Batch 3200 Loss 0.4982 Accuracy 0.0900\n",
            "Epoch 4 Batch 3250 Loss 0.4986 Accuracy 0.0901\n",
            "Epoch 4 Batch 3300 Loss 0.4973 Accuracy 0.0904\n",
            "Epoch 4 Batch 3350 Loss 0.4958 Accuracy 0.0906\n",
            "Epoch 4 Batch 3400 Loss 0.4950 Accuracy 0.0908\n",
            "Epoch 4 Batch 3450 Loss 0.4932 Accuracy 0.0911\n",
            "Epoch 4 Batch 3500 Loss 0.4919 Accuracy 0.0912\n",
            "Epoch 4 Batch 3550 Loss 0.4918 Accuracy 0.0914\n",
            "Epoch 4 Batch 3600 Loss 0.4906 Accuracy 0.0915\n",
            "Epoch 4 Batch 3650 Loss 0.4894 Accuracy 0.0916\n",
            "Epoch 4 Batch 3700 Loss 0.4895 Accuracy 0.0919\n",
            "Epoch 4 Batch 3750 Loss 0.4883 Accuracy 0.0922\n",
            "Epoch 4 Batch 3800 Loss 0.4881 Accuracy 0.0923\n",
            "Epoch 4 Batch 3850 Loss 0.4876 Accuracy 0.0925\n",
            "Epoch 4 Batch 3900 Loss 0.4870 Accuracy 0.0926\n",
            "Epoch 4 Batch 3950 Loss 0.4859 Accuracy 0.0928\n",
            "Epoch 4 Loss 0.4855 Accuracy 0.0929\n",
            "Time taken for 1 epoch: 101.2896511554718 secs\n",
            "\n",
            "Epoch 5 Batch 4000 Loss 0.4492 Accuracy 0.0989\n",
            "Epoch 5 Batch 4050 Loss 0.4335 Accuracy 0.0991\n",
            "Epoch 5 Batch 4100 Loss 0.4327 Accuracy 0.0989\n",
            "Epoch 5 Batch 4150 Loss 0.4366 Accuracy 0.0992\n",
            "Epoch 5 Batch 4200 Loss 0.4359 Accuracy 0.0994\n",
            "Epoch 5 Batch 4250 Loss 0.4348 Accuracy 0.0994\n",
            "Epoch 5 Batch 4300 Loss 0.4349 Accuracy 0.0995\n",
            "Epoch 5 Batch 4350 Loss 0.4335 Accuracy 0.0996\n",
            "Epoch 5 Batch 4400 Loss 0.4321 Accuracy 0.0998\n",
            "Epoch 5 Batch 4450 Loss 0.4314 Accuracy 0.0999\n",
            "Epoch 5 Batch 4500 Loss 0.4312 Accuracy 0.1001\n",
            "Epoch 5 Batch 4550 Loss 0.4312 Accuracy 0.1003\n",
            "Epoch 5 Batch 4600 Loss 0.4312 Accuracy 0.1004\n",
            "Epoch 5 Batch 4650 Loss 0.4306 Accuracy 0.1005\n",
            "Epoch 5 Batch 4700 Loss 0.4300 Accuracy 0.1005\n",
            "Epoch 5 Batch 4750 Loss 0.4291 Accuracy 0.1006\n",
            "Epoch 5 Batch 4800 Loss 0.4284 Accuracy 0.1008\n",
            "Epoch 5 Batch 4850 Loss 0.4277 Accuracy 0.1009\n",
            "Epoch 5 Batch 4900 Loss 0.4269 Accuracy 0.1010\n",
            "Epoch 5 Batch 4950 Loss 0.4263 Accuracy 0.1011\n",
            "Saving checkpoint for epoch 5 at /content/drive/MyDrive/Transformer/checkpoints/model\n",
            "Save checkpoints\n",
            "Epoch 5 Loss 0.4260 Accuracy 0.1012\n",
            "Time taken for 1 epoch: 102.15147423744202 secs\n",
            "\n",
            "Epoch 6 Batch 5000 Loss 0.3740 Accuracy 0.1074\n",
            "Epoch 6 Batch 5050 Loss 0.3782 Accuracy 0.1079\n",
            "Epoch 6 Batch 5100 Loss 0.3781 Accuracy 0.1077\n",
            "Epoch 6 Batch 5150 Loss 0.3789 Accuracy 0.1078\n",
            "Epoch 6 Batch 5200 Loss 0.3794 Accuracy 0.1076\n",
            "Epoch 6 Batch 5250 Loss 0.3802 Accuracy 0.1078\n",
            "Epoch 6 Batch 5300 Loss 0.3816 Accuracy 0.1080\n",
            "Epoch 6 Batch 5350 Loss 0.3808 Accuracy 0.1079\n",
            "Epoch 6 Batch 5400 Loss 0.3811 Accuracy 0.1079\n",
            "Epoch 6 Batch 5450 Loss 0.3819 Accuracy 0.1078\n",
            "Epoch 6 Batch 5500 Loss 0.3828 Accuracy 0.1079\n",
            "Epoch 6 Batch 5550 Loss 0.3823 Accuracy 0.1077\n",
            "Epoch 6 Batch 5600 Loss 0.3823 Accuracy 0.1077\n",
            "Epoch 6 Batch 5650 Loss 0.3824 Accuracy 0.1077\n",
            "Epoch 6 Batch 5700 Loss 0.3824 Accuracy 0.1077\n",
            "Epoch 6 Batch 5750 Loss 0.3821 Accuracy 0.1077\n",
            "Epoch 6 Batch 5800 Loss 0.3820 Accuracy 0.1077\n",
            "Epoch 6 Batch 5850 Loss 0.3820 Accuracy 0.1078\n",
            "Epoch 6 Batch 5900 Loss 0.3819 Accuracy 0.1079\n",
            "Epoch 6 Batch 5950 Loss 0.3820 Accuracy 0.1079\n",
            "Epoch 6 Loss 0.3819 Accuracy 0.1079\n",
            "Time taken for 1 epoch: 102.1892716884613 secs\n",
            "\n",
            "Epoch 7 Batch 6000 Loss 0.3524 Accuracy 0.1131\n",
            "Epoch 7 Batch 6050 Loss 0.3438 Accuracy 0.1125\n",
            "Epoch 7 Batch 6100 Loss 0.3477 Accuracy 0.1126\n",
            "Epoch 7 Batch 6150 Loss 0.3477 Accuracy 0.1124\n",
            "Epoch 7 Batch 6200 Loss 0.3480 Accuracy 0.1122\n",
            "Epoch 7 Batch 6250 Loss 0.3484 Accuracy 0.1121\n",
            "Epoch 7 Batch 6300 Loss 0.3499 Accuracy 0.1119\n",
            "Epoch 7 Batch 6350 Loss 0.3507 Accuracy 0.1120\n",
            "Epoch 7 Batch 6400 Loss 0.3507 Accuracy 0.1118\n",
            "Epoch 7 Batch 6450 Loss 0.3511 Accuracy 0.1119\n",
            "Epoch 7 Batch 6500 Loss 0.3521 Accuracy 0.1118\n",
            "Epoch 7 Batch 6550 Loss 0.3525 Accuracy 0.1120\n",
            "Epoch 7 Batch 6600 Loss 0.3533 Accuracy 0.1120\n",
            "Epoch 7 Batch 6650 Loss 0.3539 Accuracy 0.1121\n",
            "Epoch 7 Batch 6700 Loss 0.3541 Accuracy 0.1121\n",
            "Epoch 7 Batch 6750 Loss 0.3545 Accuracy 0.1121\n",
            "Epoch 7 Batch 6800 Loss 0.3546 Accuracy 0.1122\n",
            "Epoch 7 Batch 6850 Loss 0.3550 Accuracy 0.1122\n",
            "Epoch 7 Batch 6900 Loss 0.3552 Accuracy 0.1122\n",
            "Epoch 7 Batch 6950 Loss 0.3555 Accuracy 0.1123\n",
            "Epoch 7 Loss 0.3554 Accuracy 0.1123\n",
            "Time taken for 1 epoch: 102.02879524230957 secs\n",
            "\n",
            "Epoch 8 Batch 7000 Loss 0.3194 Accuracy 0.1174\n",
            "Epoch 8 Batch 7050 Loss 0.3213 Accuracy 0.1174\n",
            "Epoch 8 Batch 7100 Loss 0.3241 Accuracy 0.1171\n",
            "Epoch 8 Batch 7150 Loss 0.3257 Accuracy 0.1170\n",
            "Epoch 8 Batch 7200 Loss 0.3265 Accuracy 0.1167\n",
            "Epoch 8 Batch 7250 Loss 0.3273 Accuracy 0.1164\n",
            "Epoch 8 Batch 7300 Loss 0.3281 Accuracy 0.1164\n",
            "Epoch 8 Batch 7350 Loss 0.3287 Accuracy 0.1163\n",
            "Epoch 8 Batch 7400 Loss 0.3289 Accuracy 0.1163\n",
            "Epoch 8 Batch 7450 Loss 0.3300 Accuracy 0.1161\n",
            "Epoch 8 Batch 7500 Loss 0.3308 Accuracy 0.1161\n",
            "Epoch 8 Batch 7550 Loss 0.3312 Accuracy 0.1162\n",
            "Epoch 8 Batch 7600 Loss 0.3323 Accuracy 0.1161\n",
            "Epoch 8 Batch 7650 Loss 0.3329 Accuracy 0.1161\n",
            "Epoch 8 Batch 7700 Loss 0.3328 Accuracy 0.1160\n",
            "Epoch 8 Batch 7750 Loss 0.3331 Accuracy 0.1160\n",
            "Epoch 8 Batch 7800 Loss 0.3334 Accuracy 0.1160\n",
            "Epoch 8 Batch 7850 Loss 0.3333 Accuracy 0.1159\n",
            "Epoch 8 Batch 7900 Loss 0.3339 Accuracy 0.1159\n",
            "Epoch 8 Batch 7950 Loss 0.3341 Accuracy 0.1158\n",
            "Epoch 8 Loss 0.3341 Accuracy 0.1158\n",
            "Time taken for 1 epoch: 102.40746998786926 secs\n",
            "\n",
            "Epoch 9 Batch 8000 Loss 0.3007 Accuracy 0.1193\n",
            "Epoch 9 Batch 8050 Loss 0.3033 Accuracy 0.1195\n",
            "Epoch 9 Batch 8100 Loss 0.3061 Accuracy 0.1192\n",
            "Epoch 9 Batch 8150 Loss 0.3087 Accuracy 0.1194\n",
            "Epoch 9 Batch 8200 Loss 0.3107 Accuracy 0.1192\n",
            "Epoch 9 Batch 8250 Loss 0.3124 Accuracy 0.1191\n",
            "Epoch 9 Batch 8300 Loss 0.3125 Accuracy 0.1190\n",
            "Epoch 9 Batch 8350 Loss 0.3123 Accuracy 0.1189\n",
            "Epoch 9 Batch 8400 Loss 0.3127 Accuracy 0.1188\n",
            "Epoch 9 Batch 8450 Loss 0.3131 Accuracy 0.1187\n",
            "Epoch 9 Batch 8500 Loss 0.3136 Accuracy 0.1187\n",
            "Epoch 9 Batch 8550 Loss 0.3143 Accuracy 0.1186\n",
            "Epoch 9 Batch 8600 Loss 0.3153 Accuracy 0.1186\n",
            "Epoch 9 Batch 8650 Loss 0.3154 Accuracy 0.1186\n",
            "Epoch 9 Batch 8700 Loss 0.3159 Accuracy 0.1186\n",
            "Epoch 9 Batch 8750 Loss 0.3159 Accuracy 0.1186\n",
            "Epoch 9 Batch 8800 Loss 0.3162 Accuracy 0.1185\n",
            "Epoch 9 Batch 8850 Loss 0.3170 Accuracy 0.1185\n",
            "Epoch 9 Batch 8900 Loss 0.3175 Accuracy 0.1185\n",
            "Epoch 9 Loss 0.3180 Accuracy 0.1184\n",
            "Time taken for 1 epoch: 102.37824368476868 secs\n",
            "\n",
            "Epoch 10 Batch 8950 Loss 0.2878 Accuracy 0.1236\n",
            "Epoch 10 Batch 9000 Loss 0.2903 Accuracy 0.1222\n",
            "Epoch 10 Batch 9050 Loss 0.2921 Accuracy 0.1217\n",
            "Epoch 10 Batch 9100 Loss 0.2917 Accuracy 0.1217\n",
            "Epoch 10 Batch 9150 Loss 0.2924 Accuracy 0.1218\n",
            "Epoch 10 Batch 9200 Loss 0.2935 Accuracy 0.1219\n",
            "Epoch 10 Batch 9250 Loss 0.2954 Accuracy 0.1214\n",
            "Epoch 10 Batch 9300 Loss 0.2963 Accuracy 0.1212\n",
            "Epoch 10 Batch 9350 Loss 0.2974 Accuracy 0.1210\n",
            "Epoch 10 Batch 9400 Loss 0.2981 Accuracy 0.1211\n",
            "Epoch 10 Batch 9450 Loss 0.2993 Accuracy 0.1211\n",
            "Epoch 10 Batch 9500 Loss 0.2994 Accuracy 0.1210\n",
            "Epoch 10 Batch 9550 Loss 0.3001 Accuracy 0.1210\n",
            "Epoch 10 Batch 9600 Loss 0.3012 Accuracy 0.1210\n",
            "Epoch 10 Batch 9650 Loss 0.3017 Accuracy 0.1210\n",
            "Epoch 10 Batch 9700 Loss 0.3021 Accuracy 0.1210\n",
            "Epoch 10 Batch 9750 Loss 0.3030 Accuracy 0.1209\n",
            "Epoch 10 Batch 9800 Loss 0.3033 Accuracy 0.1209\n",
            "Epoch 10 Batch 9850 Loss 0.3038 Accuracy 0.1208\n",
            "Epoch 10 Batch 9900 Loss 0.3041 Accuracy 0.1208\n",
            "Saving checkpoint for epoch 10 at /content/drive/MyDrive/Transformer/checkpoints/model\n",
            "Save checkpoints\n",
            "Epoch 10 Loss 0.3047 Accuracy 0.1207\n",
            "Time taken for 1 epoch: 102.69358706474304 secs\n",
            "\n",
            "Epoch 11 Batch 9950 Loss 0.2755 Accuracy 0.1260\n",
            "Epoch 11 Batch 10000 Loss 0.2820 Accuracy 0.1253\n",
            "Epoch 11 Batch 10050 Loss 0.2820 Accuracy 0.1245\n",
            "Epoch 11 Batch 10100 Loss 0.2814 Accuracy 0.1238\n",
            "Epoch 11 Batch 10150 Loss 0.2826 Accuracy 0.1233\n",
            "Epoch 11 Batch 10200 Loss 0.2835 Accuracy 0.1231\n",
            "Epoch 11 Batch 10250 Loss 0.2843 Accuracy 0.1230\n",
            "Epoch 11 Batch 10300 Loss 0.2854 Accuracy 0.1230\n",
            "Epoch 11 Batch 10350 Loss 0.2863 Accuracy 0.1229\n",
            "Epoch 11 Batch 10400 Loss 0.2877 Accuracy 0.1229\n",
            "Epoch 11 Batch 10450 Loss 0.2892 Accuracy 0.1231\n",
            "Epoch 11 Batch 10500 Loss 0.2895 Accuracy 0.1230\n",
            "Epoch 11 Batch 10550 Loss 0.2895 Accuracy 0.1228\n",
            "Epoch 11 Batch 10600 Loss 0.2901 Accuracy 0.1227\n",
            "Epoch 11 Batch 10650 Loss 0.2905 Accuracy 0.1226\n",
            "Epoch 11 Batch 10700 Loss 0.2912 Accuracy 0.1226\n",
            "Epoch 11 Batch 10750 Loss 0.2915 Accuracy 0.1227\n",
            "Epoch 11 Batch 10800 Loss 0.2919 Accuracy 0.1226\n",
            "Epoch 11 Batch 10850 Loss 0.2928 Accuracy 0.1226\n",
            "Epoch 11 Batch 10900 Loss 0.2934 Accuracy 0.1225\n",
            "Epoch 11 Loss 0.2939 Accuracy 0.1225\n",
            "Time taken for 1 epoch: 103.24531865119934 secs\n",
            "\n",
            "Epoch 12 Batch 10950 Loss 0.2587 Accuracy 0.1268\n",
            "Epoch 12 Batch 11000 Loss 0.2700 Accuracy 0.1264\n",
            "Epoch 12 Batch 11050 Loss 0.2704 Accuracy 0.1258\n",
            "Epoch 12 Batch 11100 Loss 0.2704 Accuracy 0.1250\n",
            "Epoch 12 Batch 11150 Loss 0.2723 Accuracy 0.1253\n",
            "Epoch 12 Batch 11200 Loss 0.2728 Accuracy 0.1251\n",
            "Epoch 12 Batch 11250 Loss 0.2734 Accuracy 0.1250\n",
            "Epoch 12 Batch 11300 Loss 0.2743 Accuracy 0.1250\n",
            "Epoch 12 Batch 11350 Loss 0.2747 Accuracy 0.1247\n",
            "Epoch 12 Batch 11400 Loss 0.2757 Accuracy 0.1246\n",
            "Epoch 12 Batch 11450 Loss 0.2769 Accuracy 0.1245\n",
            "Epoch 12 Batch 11500 Loss 0.2776 Accuracy 0.1244\n",
            "Epoch 12 Batch 11550 Loss 0.2785 Accuracy 0.1243\n",
            "Epoch 12 Batch 11600 Loss 0.2795 Accuracy 0.1243\n",
            "Epoch 12 Batch 11650 Loss 0.2804 Accuracy 0.1243\n",
            "Epoch 12 Batch 11700 Loss 0.2814 Accuracy 0.1242\n",
            "Epoch 12 Batch 11750 Loss 0.2819 Accuracy 0.1242\n",
            "Epoch 12 Batch 11800 Loss 0.2824 Accuracy 0.1242\n",
            "Epoch 12 Batch 11850 Loss 0.2827 Accuracy 0.1242\n",
            "Epoch 12 Batch 11900 Loss 0.2838 Accuracy 0.1242\n",
            "Epoch 12 Loss 0.2844 Accuracy 0.1242\n",
            "Time taken for 1 epoch: 102.22939372062683 secs\n",
            "\n",
            "Epoch 13 Batch 11950 Loss 0.2581 Accuracy 0.1281\n",
            "Epoch 13 Batch 12000 Loss 0.2583 Accuracy 0.1267\n",
            "Epoch 13 Batch 12050 Loss 0.2640 Accuracy 0.1266\n",
            "Epoch 13 Batch 12100 Loss 0.2638 Accuracy 0.1267\n",
            "Epoch 13 Batch 12150 Loss 0.2631 Accuracy 0.1267\n",
            "Epoch 13 Batch 12200 Loss 0.2649 Accuracy 0.1267\n",
            "Epoch 13 Batch 12250 Loss 0.2652 Accuracy 0.1266\n",
            "Epoch 13 Batch 12300 Loss 0.2666 Accuracy 0.1265\n",
            "Epoch 13 Batch 12350 Loss 0.2672 Accuracy 0.1264\n",
            "Epoch 13 Batch 12400 Loss 0.2679 Accuracy 0.1263\n",
            "Epoch 13 Batch 12450 Loss 0.2684 Accuracy 0.1261\n",
            "Epoch 13 Batch 12500 Loss 0.2693 Accuracy 0.1260\n",
            "Epoch 13 Batch 12550 Loss 0.2704 Accuracy 0.1260\n",
            "Epoch 13 Batch 12600 Loss 0.2713 Accuracy 0.1260\n",
            "Epoch 13 Batch 12650 Loss 0.2721 Accuracy 0.1259\n",
            "Epoch 13 Batch 12700 Loss 0.2732 Accuracy 0.1259\n",
            "Epoch 13 Batch 12750 Loss 0.2738 Accuracy 0.1258\n",
            "Epoch 13 Batch 12800 Loss 0.2745 Accuracy 0.1257\n",
            "Epoch 13 Batch 12850 Loss 0.2753 Accuracy 0.1257\n",
            "Epoch 13 Batch 12900 Loss 0.2760 Accuracy 0.1256\n",
            "Epoch 13 Loss 0.2763 Accuracy 0.1256\n",
            "Time taken for 1 epoch: 102.666433095932 secs\n",
            "\n",
            "Epoch 14 Batch 12950 Loss 0.2487 Accuracy 0.1305\n",
            "Epoch 14 Batch 13000 Loss 0.2543 Accuracy 0.1297\n",
            "Epoch 14 Batch 13050 Loss 0.2557 Accuracy 0.1292\n",
            "Epoch 14 Batch 13100 Loss 0.2560 Accuracy 0.1286\n",
            "Epoch 14 Batch 13150 Loss 0.2578 Accuracy 0.1283\n",
            "Epoch 14 Batch 13200 Loss 0.2590 Accuracy 0.1281\n",
            "Epoch 14 Batch 13250 Loss 0.2586 Accuracy 0.1281\n",
            "Epoch 14 Batch 13300 Loss 0.2594 Accuracy 0.1278\n",
            "Epoch 14 Batch 13350 Loss 0.2596 Accuracy 0.1276\n",
            "Epoch 14 Batch 13400 Loss 0.2611 Accuracy 0.1275\n",
            "Epoch 14 Batch 13450 Loss 0.2616 Accuracy 0.1274\n",
            "Epoch 14 Batch 13500 Loss 0.2629 Accuracy 0.1273\n",
            "Epoch 14 Batch 13550 Loss 0.2638 Accuracy 0.1272\n",
            "Epoch 14 Batch 13600 Loss 0.2651 Accuracy 0.1272\n",
            "Epoch 14 Batch 13650 Loss 0.2665 Accuracy 0.1271\n",
            "Epoch 14 Batch 13700 Loss 0.2672 Accuracy 0.1270\n",
            "Epoch 14 Batch 13750 Loss 0.2677 Accuracy 0.1269\n",
            "Epoch 14 Batch 13800 Loss 0.2687 Accuracy 0.1267\n",
            "Epoch 14 Batch 13850 Loss 0.2695 Accuracy 0.1268\n",
            "Epoch 14 Batch 13900 Loss 0.2702 Accuracy 0.1267\n",
            "Epoch 14 Loss 0.2703 Accuracy 0.1267\n",
            "Time taken for 1 epoch: 102.49248003959656 secs\n",
            "\n",
            "Epoch 15 Batch 13950 Loss 0.2503 Accuracy 0.1297\n",
            "Epoch 15 Batch 14000 Loss 0.2465 Accuracy 0.1289\n",
            "Epoch 15 Batch 14050 Loss 0.2497 Accuracy 0.1292\n",
            "Epoch 15 Batch 14100 Loss 0.2499 Accuracy 0.1292\n",
            "Epoch 15 Batch 14150 Loss 0.2512 Accuracy 0.1292\n",
            "Epoch 15 Batch 14200 Loss 0.2528 Accuracy 0.1292\n",
            "Epoch 15 Batch 14250 Loss 0.2539 Accuracy 0.1290\n",
            "Epoch 15 Batch 14300 Loss 0.2539 Accuracy 0.1288\n",
            "Epoch 15 Batch 14350 Loss 0.2547 Accuracy 0.1287\n",
            "Epoch 15 Batch 14400 Loss 0.2557 Accuracy 0.1287\n",
            "Epoch 15 Batch 14450 Loss 0.2570 Accuracy 0.1286\n",
            "Epoch 15 Batch 14500 Loss 0.2576 Accuracy 0.1286\n",
            "Epoch 15 Batch 14550 Loss 0.2588 Accuracy 0.1283\n",
            "Epoch 15 Batch 14600 Loss 0.2597 Accuracy 0.1283\n",
            "Epoch 15 Batch 14650 Loss 0.2605 Accuracy 0.1282\n",
            "Epoch 15 Batch 14700 Loss 0.2612 Accuracy 0.1281\n",
            "Epoch 15 Batch 14750 Loss 0.2619 Accuracy 0.1281\n",
            "Epoch 15 Batch 14800 Loss 0.2625 Accuracy 0.1280\n",
            "Epoch 15 Batch 14850 Loss 0.2634 Accuracy 0.1279\n",
            "Epoch 15 Batch 14900 Loss 0.2641 Accuracy 0.1278\n",
            "Saving checkpoint for epoch 15 at /content/drive/MyDrive/Transformer/checkpoints/model\n",
            "Save checkpoints\n",
            "Epoch 15 Loss 0.2642 Accuracy 0.1278\n",
            "Time taken for 1 epoch: 102.99598002433777 secs\n",
            "\n",
            "Epoch 16 Batch 14950 Loss 0.2453 Accuracy 0.1305\n",
            "Epoch 16 Batch 15000 Loss 0.2444 Accuracy 0.1303\n",
            "Epoch 16 Batch 15050 Loss 0.2464 Accuracy 0.1301\n",
            "Epoch 16 Batch 15100 Loss 0.2466 Accuracy 0.1300\n",
            "Epoch 16 Batch 15150 Loss 0.2475 Accuracy 0.1299\n",
            "Epoch 16 Batch 15200 Loss 0.2487 Accuracy 0.1297\n",
            "Epoch 16 Batch 15250 Loss 0.2494 Accuracy 0.1296\n",
            "Epoch 16 Batch 15300 Loss 0.2505 Accuracy 0.1294\n",
            "Epoch 16 Batch 15350 Loss 0.2514 Accuracy 0.1294\n",
            "Epoch 16 Batch 15400 Loss 0.2516 Accuracy 0.1292\n",
            "Epoch 16 Batch 15450 Loss 0.2529 Accuracy 0.1292\n",
            "Epoch 16 Batch 15500 Loss 0.2529 Accuracy 0.1292\n",
            "Epoch 16 Batch 15550 Loss 0.2540 Accuracy 0.1292\n",
            "Epoch 16 Batch 15600 Loss 0.2540 Accuracy 0.1292\n",
            "Epoch 16 Batch 15650 Loss 0.2550 Accuracy 0.1290\n",
            "Epoch 16 Batch 15700 Loss 0.2560 Accuracy 0.1290\n",
            "Epoch 16 Batch 15750 Loss 0.2569 Accuracy 0.1291\n",
            "Epoch 16 Batch 15800 Loss 0.2575 Accuracy 0.1290\n",
            "Epoch 16 Batch 15850 Loss 0.2583 Accuracy 0.1290\n",
            "Epoch 16 Batch 15900 Loss 0.2589 Accuracy 0.1289\n",
            "Epoch 16 Loss 0.2589 Accuracy 0.1289\n",
            "Time taken for 1 epoch: 102.30725049972534 secs\n",
            "\n",
            "Epoch 17 Batch 15950 Loss 0.2385 Accuracy 0.1305\n",
            "Epoch 17 Batch 16000 Loss 0.2353 Accuracy 0.1309\n",
            "Epoch 17 Batch 16050 Loss 0.2379 Accuracy 0.1309\n",
            "Epoch 17 Batch 16100 Loss 0.2402 Accuracy 0.1308\n",
            "Epoch 17 Batch 16150 Loss 0.2412 Accuracy 0.1308\n",
            "Epoch 17 Batch 16200 Loss 0.2418 Accuracy 0.1305\n",
            "Epoch 17 Batch 16250 Loss 0.2433 Accuracy 0.1302\n",
            "Epoch 17 Batch 16300 Loss 0.2446 Accuracy 0.1301\n",
            "Epoch 17 Batch 16350 Loss 0.2451 Accuracy 0.1301\n",
            "Epoch 17 Batch 16400 Loss 0.2464 Accuracy 0.1300\n",
            "Epoch 17 Batch 16450 Loss 0.2480 Accuracy 0.1300\n",
            "Epoch 17 Batch 16500 Loss 0.2484 Accuracy 0.1299\n",
            "Epoch 17 Batch 16550 Loss 0.2493 Accuracy 0.1300\n",
            "Epoch 17 Batch 16600 Loss 0.2503 Accuracy 0.1300\n",
            "Epoch 17 Batch 16650 Loss 0.2514 Accuracy 0.1299\n",
            "Epoch 17 Batch 16700 Loss 0.2518 Accuracy 0.1298\n",
            "Epoch 17 Batch 16750 Loss 0.2529 Accuracy 0.1298\n",
            "Epoch 17 Batch 16800 Loss 0.2535 Accuracy 0.1298\n",
            "Epoch 17 Batch 16850 Loss 0.2540 Accuracy 0.1297\n",
            "Epoch 17 Loss 0.2544 Accuracy 0.1296\n",
            "Time taken for 1 epoch: 101.56897401809692 secs\n",
            "\n",
            "Epoch 18 Batch 16900 Loss 0.2143 Accuracy 0.1344\n",
            "Epoch 18 Batch 16950 Loss 0.2299 Accuracy 0.1311\n",
            "Epoch 18 Batch 17000 Loss 0.2307 Accuracy 0.1309\n",
            "Epoch 18 Batch 17050 Loss 0.2332 Accuracy 0.1311\n",
            "Epoch 18 Batch 17100 Loss 0.2331 Accuracy 0.1313\n",
            "Epoch 18 Batch 17150 Loss 0.2357 Accuracy 0.1311\n",
            "Epoch 18 Batch 17200 Loss 0.2374 Accuracy 0.1312\n",
            "Epoch 18 Batch 17250 Loss 0.2383 Accuracy 0.1312\n",
            "Epoch 18 Batch 17300 Loss 0.2400 Accuracy 0.1310\n",
            "Epoch 18 Batch 17350 Loss 0.2416 Accuracy 0.1311\n",
            "Epoch 18 Batch 17400 Loss 0.2424 Accuracy 0.1311\n",
            "Epoch 18 Batch 17450 Loss 0.2438 Accuracy 0.1310\n",
            "Epoch 18 Batch 17500 Loss 0.2445 Accuracy 0.1309\n",
            "Epoch 18 Batch 17550 Loss 0.2453 Accuracy 0.1307\n",
            "Epoch 18 Batch 17600 Loss 0.2461 Accuracy 0.1307\n",
            "Epoch 18 Batch 17650 Loss 0.2467 Accuracy 0.1307\n",
            "Epoch 18 Batch 17700 Loss 0.2476 Accuracy 0.1307\n",
            "Epoch 18 Batch 17750 Loss 0.2484 Accuracy 0.1306\n",
            "Epoch 18 Batch 17800 Loss 0.2490 Accuracy 0.1306\n",
            "Epoch 18 Batch 17850 Loss 0.2494 Accuracy 0.1306\n",
            "Epoch 18 Loss 0.2499 Accuracy 0.1305\n",
            "Time taken for 1 epoch: 99.55818939208984 secs\n",
            "\n",
            "Epoch 19 Batch 17900 Loss 0.2290 Accuracy 0.1310\n",
            "Epoch 19 Batch 17950 Loss 0.2233 Accuracy 0.1333\n",
            "Epoch 19 Batch 18000 Loss 0.2275 Accuracy 0.1332\n",
            "Epoch 19 Batch 18050 Loss 0.2295 Accuracy 0.1332\n",
            "Epoch 19 Batch 18100 Loss 0.2324 Accuracy 0.1329\n",
            "Epoch 19 Batch 18150 Loss 0.2334 Accuracy 0.1330\n",
            "Epoch 19 Batch 18200 Loss 0.2344 Accuracy 0.1329\n",
            "Epoch 19 Batch 18250 Loss 0.2359 Accuracy 0.1326\n",
            "Epoch 19 Batch 18300 Loss 0.2365 Accuracy 0.1325\n",
            "Epoch 19 Batch 18350 Loss 0.2375 Accuracy 0.1323\n",
            "Epoch 19 Batch 18400 Loss 0.2381 Accuracy 0.1322\n",
            "Epoch 19 Batch 18450 Loss 0.2390 Accuracy 0.1320\n",
            "Epoch 19 Batch 18500 Loss 0.2395 Accuracy 0.1320\n",
            "Epoch 19 Batch 18550 Loss 0.2402 Accuracy 0.1320\n",
            "Epoch 19 Batch 18600 Loss 0.2410 Accuracy 0.1319\n",
            "Epoch 19 Batch 18650 Loss 0.2423 Accuracy 0.1319\n",
            "Epoch 19 Batch 18700 Loss 0.2434 Accuracy 0.1317\n",
            "Epoch 19 Batch 18750 Loss 0.2441 Accuracy 0.1315\n",
            "Epoch 19 Batch 18800 Loss 0.2447 Accuracy 0.1314\n",
            "Epoch 19 Batch 18850 Loss 0.2452 Accuracy 0.1313\n",
            "Epoch 19 Loss 0.2459 Accuracy 0.1312\n",
            "Time taken for 1 epoch: 99.5472764968872 secs\n",
            "\n",
            "Epoch 20 Batch 18900 Loss 0.2335 Accuracy 0.1354\n",
            "Epoch 20 Batch 18950 Loss 0.2277 Accuracy 0.1336\n",
            "Epoch 20 Batch 19000 Loss 0.2284 Accuracy 0.1339\n",
            "Epoch 20 Batch 19050 Loss 0.2280 Accuracy 0.1338\n",
            "Epoch 20 Batch 19100 Loss 0.2289 Accuracy 0.1336\n",
            "Epoch 20 Batch 19150 Loss 0.2301 Accuracy 0.1335\n",
            "Epoch 20 Batch 19200 Loss 0.2308 Accuracy 0.1330\n",
            "Epoch 20 Batch 19250 Loss 0.2317 Accuracy 0.1328\n",
            "Epoch 20 Batch 19300 Loss 0.2334 Accuracy 0.1329\n",
            "Epoch 20 Batch 19350 Loss 0.2339 Accuracy 0.1328\n",
            "Epoch 20 Batch 19400 Loss 0.2350 Accuracy 0.1326\n",
            "Epoch 20 Batch 19450 Loss 0.2355 Accuracy 0.1324\n",
            "Epoch 20 Batch 19500 Loss 0.2361 Accuracy 0.1324\n",
            "Epoch 20 Batch 19550 Loss 0.2371 Accuracy 0.1324\n",
            "Epoch 20 Batch 19600 Loss 0.2376 Accuracy 0.1323\n",
            "Epoch 20 Batch 19650 Loss 0.2387 Accuracy 0.1322\n",
            "Epoch 20 Batch 19700 Loss 0.2399 Accuracy 0.1321\n",
            "Epoch 20 Batch 19750 Loss 0.2406 Accuracy 0.1320\n",
            "Epoch 20 Batch 19800 Loss 0.2410 Accuracy 0.1320\n",
            "Epoch 20 Batch 19850 Loss 0.2419 Accuracy 0.1319\n",
            "Saving checkpoint for epoch 20 at /content/drive/MyDrive/Transformer/checkpoints/model\n",
            "Save checkpoints\n",
            "Epoch 20 Loss 0.2426 Accuracy 0.1319\n",
            "Time taken for 1 epoch: 101.32026076316833 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL4pDyNZV8P8"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHlSFXOMVpRw",
        "outputId": "3c7f2f62-6930-4ce4-8c82-a32df3c9e438"
      },
      "source": [
        "\n",
        "# Transformer\n",
        "transformer = model.Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          source_vocab_size, target_vocab_size, \n",
        "                          pe_input=source_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)\n",
        "\n",
        "# Learning Rate\n",
        "learning_rate = model.CustomSchedule(d_model)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "\n",
        "# Loss\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "# Loss Function\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "# Metrics\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "# Checkpoint\n",
        "checkpoint_path = \"/content/drive/MyDrive/Transformer/checkpoints/model\"\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  \n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = model.create_masks(inp, tar_inp)\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp, \n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "  train_loss(loss)\n",
        "  train_accuracy(tar_real, predictions)\n",
        "\n",
        "# Initialize Weight\n",
        "initialize_weight(checkpoint_path, optimizer, transformer, MAX_LENGTH, BATCH_SIZE, use_tpu=USE_TPU)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load checkpoints successfully.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjk6F6vFWOU2"
      },
      "source": [
        "def predict(input_vec):\n",
        "\n",
        "  encoder_input = np.array([input_vec])\n",
        "\n",
        "  decoder_input = [max(train_dataset.target_token.values())-2]\n",
        "  output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "  for i in range(train_dataset.max_length):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = model.create_masks(\n",
        "        encoder_input, output)\n",
        "\n",
        "\n",
        "    predictions, attention_weights = transformer(encoder_input, \n",
        "                                                  output,\n",
        "                                                  False,\n",
        "                                                  enc_padding_mask,\n",
        "                                                  combined_mask,\n",
        "                                                  dec_padding_mask)\n",
        "    \n",
        "    predictions = predictions[: ,-1:, :]\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    if predicted_id == max(train_dataset.target_token.values())-1:\n",
        "      return tf.squeeze(output, axis=0), attention_weights\n",
        "    \n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKT-_cXKWQYq"
      },
      "source": [
        "def plot_attention_weights(attention, input_source, result, layer):\n",
        "  fig = plt.figure(figsize=(16, 8))\n",
        "  \n",
        "  sentence = tf.cast(input_source, tf.int64)\n",
        "  attention = attention[layer][0,:, :, :len(input_source)]\n",
        "  \n",
        "  for head in range(attention.shape[0]):\n",
        "    ax = fig.add_subplot(2, 4, head+1)\n",
        "    \n",
        "    ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 10}\n",
        "    \n",
        "    ax.set_xticks(range(len(sentence)))\n",
        "    ax.set_yticks(range(len(result)))\n",
        "    \n",
        "    ax.set_ylim(len(result)-1.5, -0.5)\n",
        "        \n",
        "    tmp_list = []\n",
        "    for i in sentence:\n",
        "      try:\n",
        "        tmp_list.append(train_dataset.source_index[i.numpy()])\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "    ax.set_xticklabels(tmp_list, fontdict=fontdict, rotation=90)\n",
        "    \n",
        "    ax.set_yticklabels([train_dataset.target_index[i.numpy()] for i in result \n",
        "                        if i < max(train_dataset.target_token.values()) - 1], \n",
        "                       fontdict=fontdict)\n",
        "    \n",
        "    ax.set_xlabel('Head {}'.format(head+1))\n",
        "  \n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "jHNjw3mMWSn7",
        "outputId": "bdb071d3-3424-4a8e-c5a2-886370f6d2a8"
      },
      "source": [
        "# 翻訳\n",
        "n = 1# 任意のテストデータを指定\n",
        "print(\"Input:\", ' '.join([train_dataset.source_index[i] for i in test_dataset.source_vector[n][:np.argmax(test_dataset.source_vector[n], 0)+1]]))\n",
        "\n",
        "result, attention = predict(test_dataset.source_vector[n][:np.argmax(test_dataset.source_vector[n], 0)+1])\n",
        "txt = \"\"\n",
        "\n",
        "for i in result[1:]:\n",
        "  txt+=train_dataset.target_index[i.numpy()]\n",
        "print(\"Output:\", txt)\n",
        "\n",
        "# Attention Weightをプロット\n",
        "plot_attention_weights(attention, test_dataset.source_vector[n][:np.argmax(test_dataset.source_vector[n], 0)+1], result[1:], \"decoder_layer4_block2\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> I often eat breakfast here . <end>\n",
            "Output: ここではよく朝食を食べます。\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x576 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D21K8hN6WUKZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}